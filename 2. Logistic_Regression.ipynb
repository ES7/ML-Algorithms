{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "59ba12dd",
   "metadata": {},
   "source": [
    "# Losgistic Regression\n",
    "Logistic Regression is another statistical analysis method in Machine Learning. It is used when our dependent variable is **dichotomous** or **binary**. It just means a variable that has only 2 outputs, for example, A person will survive this accident or not, The student will pass this exam or not. The outcome can either be yes or no (2 outputs). This regression technique is similar to linear regression and can be used to predict the Probabilities for classification problems.<br>\n",
    "\n",
    "### 1. Binary Logistic Regression\n",
    "Binary logistic regression is used to predict the probability of a binary outcome, such as yes or no, true or false, or 0 or 1. For example, it could be used to predict whether a patient has a disease or not, or whether a loan will be repaid or not.<br>\n",
    "\n",
    "### 2. Multinomial Logistic Regression\n",
    "Multinomial logistic regression is used to predict the probability of one of three or more possible outcomes, such as the type of product a customer will buy, the rating a customer will give a product, or the political party a person will vote for.\n",
    "\n",
    "### 3. Ordinal Logistic Regression\n",
    "Ordinal logistic regression is used to predict the probability of an outcome that falls into a predetermined order, such as the level of customer satisfaction, the severity of a disease, or the stage of cancer.\n",
    "\n",
    "## Why to use logistic regression?\n",
    "Logistic regression is only used when the dependent variable is binary and in linear regression this dependent variable is continuous. Linear regression finds a best fit line for the data.<br>\n",
    "Now, if we use linear regression to find the best fit line which aims at minimizing the distance between the predicted value and actual value, the line will be like this:<br>\n",
    "<img src=\"Images/L1.png\" width=\"400\" height=\"150\"><br>\n",
    "\n",
    "Here the threshold value is 0.5, which means if the value is greater than 0.5 then the model will predict malignant tumor (1) and if it is less than 0.5 then it predicts benign tumor (0). We add some outliers in the dataset, now this best fit line will shift to that point. Hence the line will be somewhat like this:<br>\n",
    "<img src=\"Images/L2.png\" width=\"400\" height=\"150\"><br>\n",
    "\n",
    "The blue line represents the old threshold and the yellow line represents the new threshold which is maybe 0.2 here. To keep our predictions right we had to lower our threshold value. Hence we can say that linear regression is prone to outliers. Now here if the value is greater than 0.2 then only this regression will give correct outputs.<br>\n",
    "Another problem with linear regression is that the predicted values may be out of range. We know that probability can be between 0 and 1, but if we use linear regression this probability may exceed 1 or go below 0.<br>\n",
    "To overcome these problems we use Logistic Regression, which converts this straight best fit line in linear regression to an S-curve using the **sigmoid function**, which will always give values between 0 and 1.\n",
    "\n",
    "## How does Logistic Regression works?\n",
    "**The Logistic Function**<br>\n",
    "The best fit line of linear regression is:<br>\n",
    "<img src=\"Images/L3.png\" width=\"200\" height=\"50\"><br>\n",
    "Instead of Y we will take probabilities P. But here the value of P can exceed 1 or can be below 0. To squeeze the range of P in (0,1) we take the \"odds\" of P by applying logit transformation:<br>\n",
    "<img src=\"Images/L4.png\" width=\"200\" height=\"50\"><br>\n",
    "The odds are always positive which means the range will always be **(0,+∞)**. Odds are nothing but the ratio of the probability of success and probability of failure. It is difficult to model a variable that has a restricted range. To control this we take the **log of odds** which has a range from **(-∞,+∞)**.<br>\n",
    "<img src=\"Images/L5.jpeg\" width=\"400\" height=\"150\"><br>\n",
    "\n",
    "We got the logistic function, also called a sigmoid function. The graph of a sigmoid function is as shown below. It squeezes a straight line into an S-curve.<br>\n",
    "<img src=\"Images/L6.png\" width=\"400\" height=\"150\"><br>\n",
    "\n",
    "## Cost Function in Logistic Regression\n",
    "In linear regression, we use the Mean squared error which was the difference between y_predicted and y_actual and this is derived from the maximum likelihood estimator. The graph of the cost function in linear regression is like a parabola.<br>\n",
    "In logistic regression **Yi (predicted output)** is a **non-linear** function **`(Ŷ=1​/1+e^(-z))`**. If we use this in the above MSE equation then it will give a **non-convex** graph with many **local minima**.<br>\n",
    "<img src=\"Images/L7.jpg\" width=\"400\" height=\"150\"><br>\n",
    "The problem here is that this cost function will give results with local minima, which is a big problem because then we’ll miss out on our global minima and our error will increase.<br>\n",
    "In order to solve this problem, we derive a different cost function for logistic regression called **log loss** which is also derived from the **maximum likelihood estimation method**.<br>\n",
    "<img src=\"Images/L8.png\" width=\"400\" height=\"150\"><br>\n",
    "The primary objective of Maximum Likelihood Estimation (MLE) in machine learning, is to identify parameter values that maximize the likelihood function. This function represents the joint probability density function (pdf) of our sample observations.<br>\n",
    "A random experiment whose outcomes are of two types, success S and failure F, occurring with probabilities p and q respectively is called a **Bernoulli trial**. If for this experiment a random variable X is defined such that it takes value 1 when S occurs and 0 if F occurs, then X follows a Bernoulli Distribution:<br>\n",
    "<img src=\"Images/L9.png\" width=\"200\" height=\"50\"><br>\n",
    "Bernoulli Trail for M training examples:<br>\n",
    "<img src=\"Images/L10.png\" width=\"200\" height=\"50\"><br>\n",
    "We need a value for X which will maximize this likelihood function (J). To make our calculations easier we multiply the log on both sides. The function we get is also called the log-likelihood function or sum of the log conditional probability<br>\n",
    "<img src=\"Images/L11.png\" width=\"200\" height=\"50\"><br>\n",
    "<img src=\"Images/L12.png\" width=\"400\" height=\"150\"><br>\n",
    "<img src=\"Images/L13.png\" width=\"400\" height=\"150\"><br>\n",
    "In machine learning, it is conventional to minimize a loss(error) function via gradient descent, rather than maximize an objective function via gradient ascent. If we maximize this above function then we’ll have to deal with gradient ascent to avoid this we take negative of this log so that we use gradient descent. We’ll talk more about gradient descent in a later section and then you’ll have more clarity. Also, remember, **`max(log(x)) = min(-log(x))`**<br>\n",
    "The negative of this function is our cost function and what do we want with our cost function? That it should have a minimum value. It is common practice to minimize a cost function for optimization problems; therefore, we can invert the function so that we minimize the negative log-likelihood (NLL). So in logistic regression, our cost function is:<br>\n",
    "<img src=\"Images/L14.png\" width=\"400\" height=\"150\"><br>\n",
    "\n",
    "Here y represents the actual class and log(σ(θ^T*x^i)) is the probability of that class.<br>\n",
    "* p(y) is the probability of 1.\n",
    "* 1-p(y) is the probability of 0.\n",
    "<br><img src=\"Images/L15.png\" width=\"600\" height=\"150\"><br>\n",
    "If we combine both the graphs, we will get a convex graph with only 1 local minimum and now it’ll be easy to use gradient descent here.<br>\n",
    "<img src=\"Images/L16.png\" width=\"400\" height=\"150\"><br>\n",
    "The red line here represents the 1 class (y=1), the right term of cost function will vanish. Now if the predicted probability is close to 1 then our loss will be less and when probability approaches 0, our loss function reaches infinity.<br>\n",
    "The black line represents 0 class (y=0), the left term will vanish in our cost function and if the predicted probability is close to 0 then our loss function will be less but if our probability approaches 1 then our loss function reaches infinity.<br>\n",
    "<img src=\"Images/L17.png\" width=\"200\" height=\"100\"><br>\n",
    "This cost function is also called log loss. It also ensures that as the probability of the correct answer is maximized, the probability of the incorrect answer is minimized. Lower the value of this cost function higher will be the accuracy.\n",
    "\n",
    "## Gradient Descent Optimization\n",
    "Gradient descent changes the value of our weights in such a way that it always converges to minimum point or we can also say that, it aims at finding the optimal weights which minimize the loss function of our model. It is an iterative method that finds the minimum of a function by figuring out the slope at a random point and then moving in the opposite direction.<br>\n",
    "<img src=\"Images/L18.jpg\" width=\"400\" height=\"150\"><br>\n",
    "The intuition is that if you are hiking in a canyon and trying to descend most quickly down to the river at the bottom, you might look around yourself 360 degrees, find the direction where the ground is sloping the steepest, and walk downhill in that direction.<br>\n",
    "At first gradient descent takes a random value of our parameters from our function. Now we need an algorithm that will tell us whether at the next iteration we should move left or right to reach the minimum point. The gradient descent algorithm finds the slope of the loss function at that particular point and then in the next iteration, it moves in the opposite direction to reach the minima. Since we have a convex graph now we don’t need to worry about local minima. A convex curve will always have only 1 minima.\n",
    "\n",
    "We can summarize the gradient descent algorithm as:<br>\n",
    "**`W = W - alhpa*(dJ / dW)`**<br>\n",
    "\n",
    "## Derivation of Cost Function\n",
    "<img src=\"Images/L19.png\" width=\"380\" height=\"350\"><br>\n",
    "\n",
    "Using Chain Rule break the partial derivatives of log-likehood also known as **cross entropy**<br>\n",
    "<img src=\"Images/L20.png\" width=\"300\" height=\"500\"><br>\n",
    "<img src=\"Images/L21.png\" width=\"180\" height=\"100\"><br>\n",
    "\n",
    "Where P and z are:<br>\n",
    "<img src=\"Images/L22.png\" width=\"100\" height=\"10\"><br>\n",
    "<img src=\"Images/L23.png\" width=\"200\" height=\"50\"><br>\n",
    "\n",
    "Derivative of log-likehood w.r.t P:<br>\n",
    "<img src=\"Images/L24.png\" width=\"200\" height=\"50\"><br>\n",
    "\n",
    "\n",
    "Derivative of P w.r.t z:<br>\n",
    "<img src=\"Images/L25.png\" width=\"200\" height=\"50\"><br>\n",
    "\n",
    "\n",
    "Derivative of z w.r.t W:<br>\n",
    "<img src=\"Images/L26.png\" width=\"100\" height=\"10\"><br>\n",
    "\n",
    "Final expression:<br>\n",
    "<img src=\"Images/L27.png\" width=\"300\" height=\"150\"><br>\n",
    "\n",
    "Using this we can update the W parameter:<br>\n",
    "<img src=\"Images/L28.png\" width=\"300\" height=\"150\"><br>\n",
    "\n",
    "Similarly we can update the bias term 'b'. If the slope is negative (downward slope) then our gradient descent will add some value to our new value of the parameter directing it towards the minimum point of the convex curve. Whereas if the slope is positive (upward slope) the gradient descent will minus some value to direct it towards the minimum point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9f21467a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c5b9e973",
   "metadata": {},
   "outputs": [],
   "source": [
    "bc = datasets.load_breast_cancer()\n",
    "X, y = bc.data, bc.target\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1234)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "970e35fb",
   "metadata": {},
   "source": [
    "We will loads the breast cancer dataset, which is a built-in dataset in scikit-learn containing features of cell nuclei from breast cancer biopsies and corresponding labels indicating whether the tumor is **malignant** or **benign**. Then we will split the dataset into train and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7e5caf49",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1/(1+np.exp(-x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6b72020",
   "metadata": {},
   "source": [
    "We have defined the sigmoid function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8b1012f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression():\n",
    "    def __init__(self, lr=0.01, n_iters=1000):\n",
    "        self.lr = lr\n",
    "        self.n_iters = n_iters\n",
    "        self.weights = None\n",
    "        self.bias = None\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        n_samples, n_features = X.shape\n",
    "        self.weights = np.zeros(n_features)\n",
    "        self.bias = 0\n",
    "        \n",
    "        for _ in range(self.n_iters):\n",
    "            linear_pred = np.dot(X, self.weights) + self.bias\n",
    "            pred = sigmoid(linear_pred)\n",
    "            \n",
    "            dw = (1/n_samples) * np.dot(X.T, (pred - y))\n",
    "            db = (1/n_samples) * np.sum(pred - y)\n",
    "            \n",
    "            self.weights = self.weights - self.lr * dw\n",
    "            self.bias = self.bias - self.lr * db\n",
    "            \n",
    "    def predict(self, X):\n",
    "        linear_pred = np.dot(X, self.weights) + self.bias\n",
    "        y_pred = sigmoid(linear_pred)\n",
    "        class_pred = [0 if y<=0.5 else 1 for y in y_pred]\n",
    "        return class_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5af2164b",
   "metadata": {},
   "source": [
    "The **`__init__()`** method sets up the model with 2 main parameters: **learning rate 'lr'** and **number of iterations 'n_iters'**. It also initalizes the weights and bias.<br>\n",
    "The **`fit()`** function is used to train the model on input features **X** and output **y**. It iteratively updates the weights and bias by computing the gradient and loss function (cross-entropy).<br>\n",
    "The **`predict()`** function predicts the output on test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "162ce3b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(y_pred, y_test):\n",
    "    return np.sum(y_pred == y_test)/len(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aa176a8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sayed\\AppData\\Local\\Temp\\ipykernel_22304\\4033946986.py:2: RuntimeWarning: overflow encountered in exp\n",
      "  return 1/(1+np.exp(-x))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "clf = LogisticRegression()\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "36561a08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9210526315789473\n"
     ]
    }
   ],
   "source": [
    "acc = accuracy(y_pred, y_test)\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cb6ec7b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
