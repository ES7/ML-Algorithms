{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "adc3c34b-ac4f-42f5-b58a-e054045a097b",
   "metadata": {},
   "source": [
    "## What is Random forest\n",
    "A Random Forest is like a group decision-making team in machine learning. It combines the opinions of many “trees” (individual models) to make better predictions, creating a more robust and accurate overall model.\n",
    "\n",
    "## What is Random Forest Algorithm?\n",
    "Random Forest Algorithm widespread popularity stems from its user-friendly nature and adaptability, enabling it to tackle both classification and regression problems effectively. The algorithm’s strength lies in its ability to handle complex datasets and mitigate overfitting, making it a valuable tool for various predictive tasks in machine learning.\n",
    "\n",
    "One of the most important features of the Random Forest Algorithm is that it can handle the data set containing **continuous variables**, as in the case of regression, and **categorical variables**, as in the case of classification.\n",
    "\n",
    "## Working of Random Forest Algorithm\n",
    "Suppose you are in a situation and you are confused to make decision so you ask your parents, friends, cousins and teachers to help you out. You asked different questions to each of them and they give you some suggestions. Finally after consulting various people about the course you decides to follow the method suggested my most of the people.<br>\n",
    "Random Forest combines multiple models and hence a collection of these models are used to make prediction and not just an individual model.<br>\n",
    "There are two types of methods used for combining(Ensembling) the models:\n",
    "### 1. Bagging\n",
    "It creates a different training subset from sample training data with replacement & the final output is based on majority voting. For example,  Random Forest.\n",
    "\n",
    "### 2. Boosting\n",
    "It combines weak learners into strong learners by creating sequential models such that the final model has the highest accuracy. For example, ADA BOOST, XG BOOST.\n",
    "<img src=\"Images/RF1.png\" width=\"500\" height=\"200\"><br>\n",
    "\n",
    "#### Random Forrest works on Bagging Method\n",
    "Bagging, also known as **Bootstrap Aggregation**, serves as the ensemble technique in the Random Forest algorithm. Here are the steps involved in Bagging:\n",
    "\n",
    "**1. Selection of Subset:** Bagging starts by choosing a random sample, or subset, from the entire dataset.<br>\n",
    "**2. Bootstrap Sampling:** Each model is then created from these samples, called Bootstrap Samples, which are taken from the original data with replacement. This process is known as row sampling.<br>\n",
    "**3. Bootstrapping:** The step of row sampling with replacement is referred to as bootstrapping.<br>\n",
    "**4. Independent Model Training:** Each model is trained independently on its corresponding Bootstrap Sample. This training process generates results for each model.<br>\n",
    "**5. Majority Voting:** The final output is determined by combining the results of all models through majority voting. The most commonly predicted outcome among the models is selected.<br>\n",
    "**6. Aggregation:** This step, which involves combining all the results and generating the final output based on majority voting, is known as aggregation.<br>\n",
    "<img src=\"Images/RF2.png\" width=\"400\" height=\"200\"><br>\n",
    "\n",
    "Here the bootstrap sample is taken from actual data (Bootstrap sample 01, Bootstrap sample 02, and Bootstrap sample 03) with a replacement which means there is a high possibility that each sample won’t contain unique data. The model (Model 01, Model 02, and Model 03) obtained from this bootstrap sample is trained independently. Each model generates results as shown. Now the Happy emoji has a majority when compared to the Sad emoji. Thus based on majority voting final output is obtained as Happy emoji.\n",
    "<img src=\"Images/RF3.png\" width=\"300\" height=\"100\"><br>\n",
    "\n",
    "## Boosting\n",
    "Boosting is one of the techniques that use the concept of ensemble learning. A boosting algorithm combines multiple simple models (also known as weak learners or base estimators) to generate the final output. It is done by building a model by using weak models in series.\n",
    "\n",
    "There are several boosting algorithms; **AdaBoost** was the first really successful boosting algorithm that was developed for the purpose of binary classification. AdaBoost is an abbreviation for **Adaptive Boosting** and is a prevalent boosting technique that combines multiple \"weak classifiers\" into a single \"strong classifier\". There are Other Boosting techniques.\n",
    "\n",
    "## Steps Involved in Random Forest Algorithm\n",
    "**Step 1:** In the Random forest model, a subset of data points and a subset of features is selected for constructing each decision tree. Simply put, n random records and m features are taken from the data set having k number of records.<br>\n",
    "**Step 2:** Individual decision trees are constructed for each sample.<br>\n",
    "**Step 3:** Each decision tree will generate an output.<br>\n",
    "**Step 4:** Final output is considered based on Majority Voting or Averaging for Classification and regression, respectively.<br>\n",
    "\n",
    "### Example\n",
    "Consider the fruit basket as the data as shown in the figure below. Now n number of samples are taken from the fruit basket, and an individual decision tree is constructed for each sample. Each decision tree will generate an output, as shown in the figure. The final output is considered based on majority voting. In the below figure, you can see that the majority decision tree gives output as an apple when compared to a banana, so the final output is taken as an apple.<br>\n",
    "<img src=\"Images/RF4.png\" width=\"500\" height=\"200\"><br>\n",
    "\n",
    "## Important Features of Random Forest\n",
    "* **Diversity:** Not all attributes/variables/features are considered while making an individual tree; each tree is different.\n",
    "Immune to the curse of dimensionality: Since each tree does not consider all the features, the feature space is reduced.\n",
    "* **Parallelization:** Each tree is created independently out of different data and attributes. This means we can fully use the CPU to build random forests.\n",
    "* **Train-Test split:** In a random forest, we don’t have to segregate the data for train and test as there will always be 30% of the data which is not seen by the decision tree.\n",
    "* **Stability:** Stability arises because the result is based on majority voting/ averaging.\n",
    "\n",
    "## Difference Between Decision Tree and Random Forest\n",
    "Random forest is a collection of decision trees; still, there are a lot of differences in their behavior.\n",
    "\n",
    "### Decision Tree:\n",
    "1. Decision trees normally suffer from the problem of overfitting if it’s allowed to grow without any control.\n",
    "2. A single decision tree is faster in computation.\n",
    "3. When a data set with features is taken as input by a decision tree, it will formulate some rules to make predictions.\n",
    "\n",
    "### Random Forrest:\n",
    "1. Random forests are created from subsets of data, and the final output is based on average or majority ranking; hence the problem of overfitting is taken care of..\n",
    "2. It is comparatively slower.\n",
    "3. Random forest randomly selects observations, builds a decision tree, and takes the average result. It doesn’t use any set of formulas.\n",
    "\n",
    "## Important Hyperparameters in Random Forest\n",
    "Hyperparameters are used in random forests to either enhance the performance and predictive power of models or to make the model faster.\n",
    "\n",
    "### Hyperparameters to Increase the Predictive Power\n",
    "* **n_estimators:** Number of trees the algorithm builds before averaging the predictions.\n",
    "* **max_features:** Maximum number of features random forest considers splitting a node.\n",
    "* **mini_sample_leaf:** Determines the minimum number of leaves required to split an internal node.\n",
    "* **criterion:** How to split the node in each tree? (Entropy/Gini impurity/Log Loss)\n",
    "* **max_leaf_nodes:** Maximum leaf nodes in each tree\n",
    "\n",
    "### Hyperparameters to Increase the Speed\n",
    "* **n_jobs:** it tells the engine how many processors it is allowed to use. If the value is 1, it can use only one processor, but if the value is -1, there is no limit.\n",
    "* **random_state:** controls randomness of the sample. The model will always produce the same results if it has a definite value of random state and has been given the same hyperparameters and training data.\n",
    "* **oob_score:** OOB means out of the bag. It is a random forest cross-validation method. In this, one-third of the sample is not used to train the data; instead used to evaluate its performance. These samples are called out-of-bag samples.\n",
    "\n",
    "## Advantages\n",
    "* It can be used in classification and regression problems.\n",
    "* It solves the problem of overfitting as output is based on majority voting or averaging.\n",
    "* It performs well even if the data contains null/missing values.\n",
    "* Each decision tree created is independent of the other; thus, it shows the property of parallelization.\n",
    "* It is highly stable as the average answers given by a large number of trees are taken.\n",
    "* It maintains diversity as all the attributes are not considered while making each decision tree though it is not true in all cases.\n",
    "* It is immune to the curse of dimensionality. Since each tree does not consider all the attributes, feature space is reduced.\n",
    "* We don’t have to segregate data into train and test as there will always be 30% of the data, which is not seen by the decision tree made out of bootstrap.\n",
    "\n",
    "## Disadvantages\n",
    "* Random forest is highly complex compared to decision trees, where decisions can be made by following the path of the tree.\n",
    "* Training time is more than other models due to its complexity. Whenever it has to make a prediction, each decision tree has to generate output for the given input data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73b75354-1880-4269-ba83-b8597a0ae04b",
   "metadata": {},
   "source": [
    "# Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "26669536-ac8b-4aa0-9757-ccd6d1202e2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import model_selection\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c02bd320-fa8a-4dd1-a5ce-e8c077516ef9",
   "metadata": {},
   "source": [
    "Here I have imported **warnings** library to avoid warning messages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "37ac10bf-22ec-450a-a66a-e17f4124b78a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Leaf:\n",
    "    def __init__(self, value):\n",
    "        self.value = value\n",
    "\n",
    "    def predict(self, row):\n",
    "        return self.value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69583049-2117-42c6-97db-87ab826da2d6",
   "metadata": {},
   "source": [
    "This class represents a leaf in decision tree.<br>\n",
    "The **`__init__()`** method is a constructor method which initializes a Leaf object with a value. This value is typically the predicted outcome or class label associated with the leaf node.<br>\n",
    "The **`predict()`** method predict method takes a row as input and returns the value of the leaf node. In a decision tree, when a new data point (represented by row) reaches a leaf node during prediction, the predict method is called to return the predicted value for that data point.<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6d595038-1208-435f-b4c6-0b13f4326019",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node:\n",
    "    def __init__(self, level, split_feature, split_value, left_node=None, right_node=None):\n",
    "        self.level = level\n",
    "        self.split_feature = split_feature\n",
    "        self.split_value = split_value\n",
    "        self.left_node = left_node\n",
    "        self.right_node = right_node\n",
    "\n",
    "    def predict(self, row):\n",
    "        if row[self.split_feature] >= self.split_value:\n",
    "            return self.right_node.predict(row)\n",
    "        return self.left_node.predict(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4af3781-b3d3-42f3-9a7f-357db123b9c3",
   "metadata": {},
   "source": [
    "The Node class represents a node in a decision tree that contains a splitting criterion and references to its left and right child nodes.<br>\n",
    "The **`__init__()`** method is a constructor method that initializes a Node object with the following attributes:\n",
    "* **level:** The level of the node in the decision tree.\n",
    "* **split_feature:** The index of the feature used for splitting at this node.\n",
    "* **split_value:** The value of the feature used for splitting at this node.\n",
    "* **left_node:** Reference to the left child node.\n",
    "* **right_node:** Reference to the right child node.\n",
    "\n",
    "The **`predict()`** method takes a row as input and returns the predicted value by traversing the decision tree. If the feature value of the row is greater than or equal to the split_value, it follows the right child node; otherwise, it follows the left child node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "336ab072-7e06-4884-a2a8-53a853161ffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecisionTreeClassifier:\n",
    "    def __init__(self, max_depth):\n",
    "        self.max_depth = max_depth\n",
    "        self.root = None\n",
    "    def set_root(self, node):\n",
    "        if self.root == None:\n",
    "            self.root = node\n",
    "    \n",
    "    def class_counts(self, y):\n",
    "        values, counts = np.unique(y, return_counts=True)\n",
    "        return values, counts\n",
    "\n",
    "    def calc_popular_class(self, y):\n",
    "        values, counts = self.class_counts(y)\n",
    "        idx = np.argmax(counts)\n",
    "        popular_class = values[idx]\n",
    "        return popular_class\n",
    "\n",
    "    def calc_gini(self, y):\n",
    "        values, counts = self.class_counts(y)\n",
    "        class_probabilities = counts/float(len(y))\n",
    "        return 1-np.sum(class_probabilities**2, axis=0)\n",
    "\n",
    "    def features_to_check(self, num_features):\n",
    "        num_features_to_check = int(math.sqrt(num_features))\n",
    "        idxs = np.random.randint(0, num_features, size=num_features_to_check)\n",
    "        return idxs\n",
    "\n",
    "    def get_best_split(self, X, y):\n",
    "        num_features = X.shape[1]\n",
    "        num_rows = len(y)\n",
    "        best_split_feature = 0\n",
    "        best_split_value = 0\n",
    "        best_gini = 1\n",
    "\n",
    "        for feature in self.features_to_check(num_features-1):\n",
    "            values = np.unique(X[:, feature])\n",
    "\n",
    "            for val in values:\n",
    "                right_rows, right_labels, left_rows, left_labels = self.data_split(X, y, feature, val)\n",
    "                p = float(len(right_rows))/num_rows\n",
    "                average_gini = p*self.calc_gini(right_labels)/num_rows + (1-p)*self.calc_gini(left_labels)/num_rows\n",
    "\n",
    "                if average_gini < best_gini:\n",
    "                    best_gini = average_gini\n",
    "                    best_split_feature, best_split_value = feature, val\n",
    "        \n",
    "        return best_split_feature, best_split_value, best_gini\n",
    "\n",
    "    def data_split(self, X, y, split_feature, split_value):\n",
    "        idx_right_subtree = X[:, split_feature] >= split_value\n",
    "        right_subtree = X[idx_right_subtree]\n",
    "        right_subtree_labels = y[idx_right_subtree]\n",
    "\n",
    "        idx_left_subtree = X[:, split_feature] < split_value\n",
    "        left_subtree = X[idx_left_subtree]\n",
    "        left_subtree_labels = y[idx_left_subtree]\n",
    "\n",
    "        return right_subtree, right_subtree_labels, left_subtree, left_subtree_labels\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.set_root(self.split_node(X, y))\n",
    "    \n",
    "    def split_node(self, X, y, node_level=0):\n",
    "        node_level += 1\n",
    "        \n",
    "        if len(y) == 1:\n",
    "            return Leaf(y[0])\n",
    "        split_feature, split_value, gini = self.get_best_split(X, y)\n",
    "\n",
    "        if gini == 0.0 or self.max_depth < node_level:\n",
    "            popular_class = self.calc_popular_class(y)\n",
    "            return Leaf(popular_class)\n",
    "        right_subtree, right_subtree_labels, left_subtree, left_subtree_labels = self.data_split(X, y, split_feature, split_value)\n",
    "\n",
    "        if len(right_subtree_labels) == 1:\n",
    "            return Leaf(right_subtree_labels[0])\n",
    "        if len(left_subtree_labels) == 1:\n",
    "            return Leaf(left_subtree_labels[0])\n",
    "        right_node = self.split_node(right_subtree, right_subtree_labels, node_level)\n",
    "        left_node = self.split_node(left_subtree, left_subtree_labels, node_level)\n",
    "        return Node(node_level, split_feature, split_value, left_node, right_node)\n",
    "\n",
    "    def predict_labels(self, X_test):\n",
    "        y_probs = []\n",
    "        for row in X_test:\n",
    "            y_probs.append(self.root.predict(row))\n",
    "        return np.asarray(y_probs)\n",
    "\n",
    "    def get_accuracy(self, y, y_probs):\n",
    "        correct = y == y_probs\n",
    "        acc = (np.sum(correct)/float(len(y)))*100.0\n",
    "        return acc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "085bcb6a-3432-42e3-99be-f4504bdd2811",
   "metadata": {},
   "source": [
    "The **DecisionTreeClassifier()** class represents a decision tree model for classification tasks.<br>\n",
    "\n",
    "The **`__init__()`** method initializes a DecisionTreeClassifier object with the **max_depth** parameter, which specifies the maximum depth of the decision tree. It also initializes the **root** attribute to None, which will later hold the root node of the decision tree.<br>\n",
    "The **`set_root()`** method sets the root node of the decision tree if it is not already set.<br>\n",
    "The **`class_counts()`** method calculates the counts of unique classes in the target variable y and returns the values and their corresponding counts.<br>\n",
    "The **`calc_popular_class()`** method calculates the most popular class in the target variable y, which is the class with the highest frequency, and returns it as the predicted class for a leaf node.<br>\n",
    "The **`calc_gini()`** method calculates the **Gini impurity** of a node based on the target variable y. Gini impurity is a measure of how often a randomly chosen element would be incorrectly labeled if it was randomly labeled according to the distribution of labels in the node. Lower Gini impurity indicates better purity (i.e., more homogeneous class distribution).<br>\n",
    "The **`features_to_check()`** method randomly selects a subset of features to consider for splitting a node. It is a common technique in decision tree algorithms to reduce overfitting by considering only a subset of features at each node.<br>\n",
    "The **`get_best_split()`** method finds the best feature and value to split a node based on the Gini impurity. It iterates over each feature and value combination to find the split that results in the lowest average Gini impurity for the child nodes.<br>\n",
    "The **`data_split()`** method splits the data X and labels y into two subsets based on a split feature and value.<br>\n",
    "The **`fit()`** method fits the decision tree to the training data by recursively splitting nodes until a stopping criterion is met, such as reaching the maximum depth or achieving perfect purity.<br>\n",
    "The **`split_node()`** method recursively splits nodes to build the decision tree. It checks for the stopping criteria and splits nodes based on the best split found by get_best_split.<br>\n",
    "The **`predict_labels()`** method predicts the class labels for a given set of input data X_test by traversing the decision tree and returning the predicted class for each input.<br>\n",
    "The **`get_accuracy()`** method calculates the accuracy of the model by comparing the predicted labels y_probs with the actual labels y and computing the percentage of correct predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2d09c194-095f-4a54-a6c2-cff8a84e62ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomForestClassifier():\n",
    "    def __init__(self):\n",
    "        self.forest = []\n",
    "    \n",
    "    def create_subsample(self, X, y, a=0.25):\n",
    "        n = len(y)\n",
    "        n_tag = int(a*n)\n",
    "        idx = np.random.randint(0, n, size=n_tag)\n",
    "        X_subsample = X[idx]\n",
    "        y_subsample = y[idx]\n",
    "        return X_subsample, y_subsample\n",
    "\n",
    "    def fit(self, X, y, T=300, max_depth=4):\n",
    "        for i in range(0,T):\n",
    "            X_subsample, y_subsample = self.create_subsample(X, y)\n",
    "            tree = DecisionTreeClassifier(max_depth)\n",
    "            tree.fit(X_subsample, y_subsample)\n",
    "            self.forest.append(tree)\n",
    "\n",
    "    def calc_popular_class(self, y):\n",
    "        values, counts = np.unique(y, return_counts=True)\n",
    "        idx = np.argmax(counts)\n",
    "        popular_class = values[idx]\n",
    "        return popular_class\n",
    "\n",
    "    def bagging_predict(self, X_test):\n",
    "        predictions = []\n",
    "\n",
    "        for row in X_test:\n",
    "            all_trees_preds = np.asarray([tree.root.predict(row) for tree in self.forest])\n",
    "            if len(all_trees_preds)>0:\n",
    "                predictions.append(self.calc_popular_class(all_trees_preds))\n",
    "        return np.asarray(predictions)\n",
    "\n",
    "    def get_accuracy(self, y, y_probs):\n",
    "        correct = y == y_probs\n",
    "        acc = (np.sum(correct)/float(len(y)))*100.0\n",
    "        return acc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd4a8b5f-04f6-4901-829b-23c0a58722dc",
   "metadata": {},
   "source": [
    "The **RandomForestClassifier** class implements a random forest model for classification tasks. It builds an ensemble of decision trees and aggregates their predictions to make the final classification.<br>\n",
    "\n",
    "The **`__init__()`** method initializes the forest attribute as an empty list, which will hold the individual decision trees in the random forest.<br>\n",
    "The **`create_subsample()`** method creates a random subsample of the dataset X and target variable y for training each decision tree in the forest. It randomly selects a subset of rows (a fraction of the total rows) and returns the subsampled dataset.<br>\n",
    "The **`fit()`** method fits the random forest model to the training data X and target variable y. It iteratively creates a subsample of the training data, trains a decision tree on the subsample, and adds the tree to the forest. The process is repeated T times (default is 300) to create a forest of decision trees.<br>\n",
    "The **`calc_popular_class()`** method calculates the most popular class in a set of predictions y. It is used for aggregating the predictions of individual decision trees in the forest.<br>\n",
    "The **`bagging_predict()`** method makes predictions for a given set of input data X_test by aggregating the predictions of all decision trees in the forest. For each input row, it collects the predictions of all trees and calculates the most popular class among them as the final prediction.<br>\n",
    "The **`get_accuracy()`** method calculates the accuracy of the model by comparing the predicted labels y_probs with the actual labels y and computing the percentage of correct predictions.<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5518b11d-63b2-4af8-ba3b-92767096340d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "from sklearn.model_selection import train_test_split\n",
    "digit = load_digits()\n",
    "X = digit.data\n",
    "y = digit.target\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4e6f8c71-79e1-42f7-9e3a-8898d45b804b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "91.38888888888889\n"
     ]
    }
   ],
   "source": [
    "clf = RandomForestClassifier()\n",
    "clf.fit(X_train, y_train)\n",
    "preds = clf.bagging_predict(X_test)\n",
    "acc = clf.get_accuracy(y_test, preds)\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b05c8e7-862a-4d5d-b61d-76aadec84803",
   "metadata": {},
   "source": [
    "# Random Forest Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7693914b-f5e6-40e6-89ce-072c6d88513e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecisionTreeRegressor:\n",
    "    def __init__(self, max_depth):\n",
    "        self.max_depth = max_depth\n",
    "        self.root = None\n",
    "    \n",
    "    def set_root(self, node):\n",
    "        if self.root == None:\n",
    "            self.root = node\n",
    "    \n",
    "    def class_counts(self, y):\n",
    "        values, counts = np.unique(y, return_counts=True)\n",
    "        return values, counts\n",
    "\n",
    "    def calc_popular_class(self, y):\n",
    "        return np.mean(y)\n",
    "\n",
    "    def calc_mse(self, y):\n",
    "        return np.mean((y-np.mean(y))**2)\n",
    "\n",
    "    def features_to_check(self, num_features):\n",
    "        num_features_to_check = int(math.sqrt(num_features))\n",
    "        idxs = np.random.randint(0, num_features, size=num_features_to_check)\n",
    "        return idxs\n",
    "\n",
    "    def get_best_split(self, X, y):\n",
    "        num_features = X.shape[1]\n",
    "        num_rows = len(y)\n",
    "        best_split_feature = 0\n",
    "        best_split_value = 0\n",
    "        best_mse = float('inf')\n",
    "\n",
    "        for feature in self.features_to_check(num_features-1):\n",
    "            values = np.unique(X[:, feature])\n",
    "\n",
    "            for val in values:\n",
    "                right_rows, right_labels, left_rows, left_labels = self.data_split(X, y, feature, val)\n",
    "                p = float(len(right_rows))/num_rows\n",
    "                average_mse = p*self.calc_mse(right_labels)/num_rows + (1-p)*self.calc_mse(left_labels)/num_rows\n",
    "\n",
    "                if average_mse < best_mse:\n",
    "                    best_mse = average_mse\n",
    "                    best_split_feature, best_split_value = feature, val\n",
    "        \n",
    "        return best_split_feature, best_split_value, best_mse\n",
    "\n",
    "    def data_split(self, X, y, split_feature, split_value):\n",
    "        idx_right_subtree = X[:, split_feature] >= split_value\n",
    "        right_subtree = X[idx_right_subtree]\n",
    "        right_subtree_labels = y[idx_right_subtree]\n",
    "\n",
    "        idx_left_subtree = X[:, split_feature] < split_value\n",
    "        left_subtree = X[idx_left_subtree]\n",
    "        left_subtree_labels = y[idx_left_subtree]\n",
    "\n",
    "        return right_subtree, right_subtree_labels, left_subtree, left_subtree_labels\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.set_root(self.split_node(X, y))\n",
    "    \n",
    "    def split_node(self, X, y, node_level=0):\n",
    "        node_level += 1\n",
    "        \n",
    "        if len(y) == 1:\n",
    "            return Leaf(y[0])\n",
    "        split_feature, split_value, mse = self.get_best_split(X, y)\n",
    "\n",
    "        if mse == 0.0 or self.max_depth < node_level:\n",
    "            popular_class = self.calc_popular_class(y)\n",
    "            return Leaf(popular_class)\n",
    "        right_subtree, right_subtree_labels, left_subtree, left_subtree_labels = self.data_split(X, y, split_feature, split_value)\n",
    "\n",
    "        if len(right_subtree_labels) == 1:\n",
    "            return Leaf(right_subtree_labels[0])\n",
    "        if len(left_subtree_labels) == 1:\n",
    "            return Leaf(left_subtree_labels[0])\n",
    "        right_node = self.split_node(right_subtree, right_subtree_labels, node_level)\n",
    "        left_node = self.split_node(left_subtree, left_subtree_labels, node_level)\n",
    "        return Node(node_level, split_feature, split_value, left_node, right_node)\n",
    "\n",
    "    def predict(self, X_test):\n",
    "        y_preds = []\n",
    "        for row in X_test:\n",
    "            pred = self.root.predict(row)\n",
    "            if not np.isfinite(pred):\n",
    "                pred = np.nanmean(y)\n",
    "            y_preds.append(pred)\n",
    "        return np.asarray(y_preds)\n",
    "\n",
    "    def get_mse(self, y, y_preds):\n",
    "        return np.mean((y-y_preds)**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca9e3866-94d3-426d-9944-4b9bca29cadf",
   "metadata": {},
   "source": [
    "This **DecisionTreeRegressor** class implements a decision tree model for regression tasks. It builds a binary tree where each internal node represents a decision based on a feature value, and each leaf node represents the output value (mean of the target variable) for the corresponding subset of data.\n",
    "\n",
    "The **`__init__()`** method initializes the decision tree with a specified maximum depth max_depth and sets the root node to None.<br>\n",
    "The **`set_root()`** method sets the root node of the decision tree if it is not already set.<br>\n",
    "The **`class_counts()`** method calculates the counts of unique classes in the target variable y and returns the values and their corresponding counts.<br>\n",
    "The **`calc_popular_class()`** method calculates the mean of a given set of values y, which is used when determining the output value for a leaf node.<br>\n",
    "The **`calc_mse()`** method calculates the mean squared error for a given set of values y, which is used to evaluate the quality of a split.<br>\n",
    "The **`features_to_check()`** method returns a random subset of feature indices to consider when finding the best split. This helps in creating random forests with diverse trees.<br>\n",
    "The **`get_best_split()`** method finds the best feature and value to split a node based on the Gini impurity. It iterates over each feature and value combination to find the split that results in the lowest average Gini impurity for the child nodes.<br>\n",
    "The **`data_split()`** method splits the data X and labels y into two subsets based on a split feature and value.<br>\n",
    "The **`fit()`** methods fits the decision tree to the training data X and target variable y by calling the split_node method to recursively split the data and create the tree.<br>\n",
    "The **`split_node()`** method recursively splits the data at each node based on the best feature and value to minimize the mean squared error (mse). It uses the get_best_split method to find the best split and creates child nodes accordingly.<br>\n",
    "The **`predict()`** method predicts the target variable for a given set of input features X_test by traversing the decision tree from the root node to a leaf node. If a leaf node's prediction is not finite (due to no data in that leaf), it replaces the prediction with the mean of the target variable y."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fbb8b432-fffb-41e3-bdcc-d22a48dc0604",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomForestRegressor():\n",
    "    def __init__(self):\n",
    "        self.forest = []\n",
    "    \n",
    "    def create_subsample(self, X, y, a=0.25):\n",
    "        n = len(y)\n",
    "        n_tag = int(a*n)\n",
    "        idx = np.random.randint(0, n, size=n_tag)\n",
    "        X_subsample = X[idx]\n",
    "        y_subsample = y[idx]\n",
    "        return X_subsample, y_subsample\n",
    "\n",
    "    def fit(self, X, y, T=300, max_depth=4):\n",
    "        for i in range(T):\n",
    "            X_subsample, y_subsample = self.create_subsample(X, y)\n",
    "            tree = DecisionTreeRegressor(max_depth)\n",
    "            tree.fit(X_subsample, y_subsample)\n",
    "            self.forest.append(tree)\n",
    "\n",
    "    def predict(self, X):\n",
    "        predictions = []\n",
    "        for tree in self.forest:\n",
    "            predictions.append(tree.predict(X))\n",
    "        return np.mean(predictions, axis=0)\n",
    "\n",
    "    def get_mse(self, y_true, y_pred):\n",
    "            return np.mean((y_true - y_pred)**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da344658-85ba-4047-8c9e-4cea7ce2abab",
   "metadata": {},
   "source": [
    "This **RandomForestRegressor** class implements a random forest model for regression tasks. Random forests are an ensemble learning method that builds multiple decision trees during training and outputs the mean prediction of the individual trees for regression.\n",
    "\n",
    "The **`__init__()`** method initializes the random forest regressor with an empty forest list to store the individual decision trees.<br>\n",
    "The **`create_subsample()`** method creates a random subsample of the dataset X and target variable y with a specified sampling ratio a. It randomly selects a*n samples from the dataset, where n is the total number of samples in y.<br>\n",
    "The **`fit()`** method fits the random forest to the training data X and target variable y by creating T decision trees using the DecisionTreeRegressor class. Each tree is trained on a different random subsample of the data.<br>\n",
    "The **`predict()`** method predicts the target variable for a given set of input features X by averaging the predictions of all the decision trees in the forest.<br>\n",
    "The **`get_mse()`** method calculates the mean squared error (MSE) between the true target variable y_true and the predicted values y_pred."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8eca35ad-b1dc-4e9e-9479-07692662f58c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_diabetes\n",
    "diabetes = load_diabetes()\n",
    "X_diabetes, y_diabetes = diabetes.data, diabetes.target\n",
    "\n",
    "X_diabetes_train, X_diabetes_test, y_diabetes_train, y_diabetes_test = train_test_split(X_diabetes, y_diabetes, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "631ea8bd-78c3-44fc-ae95-634ddaf333a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3370.6018386463347\n"
     ]
    }
   ],
   "source": [
    "regressor = RandomForestRegressor()\n",
    "regressor.fit(X_diabetes_train, y_diabetes_train)\n",
    "\n",
    "y_diabetes_pred = regressor.predict(X_diabetes_test)\n",
    "\n",
    "mse = regressor.get_mse(y_diabetes_test, y_diabetes_pred)\n",
    "print(mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7399587b-7f7e-4c96-b105-1eae38ecf03f",
   "metadata": {},
   "source": [
    "# Using Sklearn Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "27545022-ae61-4000-90c7-7c676a85c1a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from sklearn.datasets import load_digits, load_diabetes\n",
    "from sklearn.metrics import accuracy_score, mean_squared_error\n",
    "\n",
    "digit = load_digits()\n",
    "diabetes = load_diabetes()\n",
    "\n",
    "X_digit_train, X_digit_test, y_digit_train, y_digit_test = train_test_split(digit.data, digit.target, test_size=0.2, random_state=42)\n",
    "X_diabetes_train, X_diabetes_test, y_diabetes_train, y_diabetes_test = train_test_split(diabetes.data, diabetes.target, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "733aa21a-51f6-44f5-9fab-47f8c6028d78",
   "metadata": {},
   "source": [
    "## Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "98419185-a787-43da-9c1d-b0f86bdf5836",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Digit Classification Accuracy: 0.9722222222222222\n"
     ]
    }
   ],
   "source": [
    "clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "clf.fit(X_digit_train, y_digit_train)\n",
    "y_digit_pred = clf.predict(X_digit_test)\n",
    "digit_accuracy = accuracy_score(y_digit_test, y_digit_pred)\n",
    "print(f\"Digit Classification Accuracy: {digit_accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e62c6938-acf6-4a45-afb6-d6644a2fed49",
   "metadata": {},
   "source": [
    "## Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "98170cef-635b-402f-8f00-dfb6def8725e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Diabetes Regression Mean Squared Error: 2952.0105887640448\n"
     ]
    }
   ],
   "source": [
    "reg = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "reg.fit(X_diabetes_train, y_diabetes_train)\n",
    "y_diabetes_pred = reg.predict(X_diabetes_test)\n",
    "diabetes_mse = mean_squared_error(y_diabetes_test, y_diabetes_pred)\n",
    "print(f\"Diabetes Regression Mean Squared Error: {diabetes_mse}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d736eaf7-6e34-426f-9baa-587903416d75",
   "metadata": {},
   "source": [
    "# Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "03459c08-ced8-449b-a4ff-0a7fa44f5387",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters for classifier model: {'max_depth': None, 'max_features': 'sqrt', 'min_samples_leaf': 1, 'min_samples_split': 5, 'n_estimators': 200}\n",
      "Best Score: 0.9749516066589237\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [None, 10, 20],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'max_features': ['auto', 'sqrt', 'log2']\n",
    "}\n",
    "\n",
    "# Create the GridSearchCV object\n",
    "rf_clf = RandomForestClassifier(random_state=42)\n",
    "grid_search = GridSearchCV(rf_clf, param_grid, cv=5, n_jobs=-1)\n",
    "\n",
    "# Perform the grid search\n",
    "grid_search.fit(X_digit_train, y_digit_train)\n",
    "\n",
    "# Get the best parameters and best score\n",
    "best_params = grid_search.best_params_\n",
    "best_score = grid_search.best_score_\n",
    "\n",
    "print(\"Best Parameters for classifier model:\", best_params)\n",
    "print(\"Best Score:\", best_score)\n",
    "\n",
    "# Use the best model for predictions\n",
    "best_rf_clf = grid_search.best_estimator_\n",
    "y_pred = best_rf_clf.predict(X_digit_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f9f1591-e7ac-4095-9d86-af24e8ac1314",
   "metadata": {},
   "source": [
    "The **GridSearchCV** is used for performing hyperparameter tuning through an exhaustive search over specified parameter values for an estimator.<br>\n",
    "The **param_grid** dictionary defines the hyperparameters and their corresponding values to be tuned. It includes parameters like **n_estimators, max_depth, min_samples_split, min_samples_leaf,** and **max_features**.<br>\n",
    "The RandomForestClassifier is initialized with a random_state for reproducibility. Then, a GridSearchCV object is created with the classifier, the parameter grid, **5-fold cross-validation (cv=5)**, and **parallel processing (n_jobs=-1)**.<br>\n",
    "The fit method of the GridSearchCV object is called with the training data (X_digit_train and y_digit_train). This step performs an exhaustive search over the hyperparameter values specified in param_grid and evaluates the model performance using cross-validation.<br>\n",
    "After the grid search is complete, the best parameters **(best_params_)** and best score **(best_score_)** are obtained from the GridSearchCV object. These values represent the hyperparameters that yielded the highest cross-validated score during the grid search and printed to the console.<br>\n",
    "Finally, the best estimator **(best_rf_clf)** from the grid search is used to make predictions on the test data **(X_digit_test)**, and the predicted labels **(y_pred)** are obtained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cc50f144-298e-43a1-afd2-cee1c8464fda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters for regressor model: {'max_depth': 10, 'max_features': 'sqrt', 'min_samples_leaf': 1, 'min_samples_split': 10, 'n_estimators': 200}\n",
      "Best Score: 0.4266408924229098\n"
     ]
    }
   ],
   "source": [
    "rf_reg = RandomForestRegressor(random_state=42)\n",
    "grid_search = GridSearchCV(rf_reg, param_grid, cv=5, n_jobs=-1)\n",
    "\n",
    "# Perform the grid search\n",
    "grid_search.fit(X_diabetes_train, y_diabetes_train)\n",
    "\n",
    "# Get the best parameters and best score\n",
    "best_params = grid_search.best_params_\n",
    "best_score = grid_search.best_score_\n",
    "\n",
    "print(\"Best Parameters for regressor model:\", best_params)\n",
    "print(\"Best Score:\", best_score)\n",
    "\n",
    "# Use the best model for predictions\n",
    "best_rf_reg = grid_search.best_estimator_\n",
    "y_pred = best_rf_reg.predict(X_diabetes_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93e432c2-4175-47cb-a518-45dddc1c457c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
